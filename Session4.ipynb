{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c818d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PySpark: Spark's API for Python.\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df979e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed90803",
   "metadata": {},
   "source": [
    "### What is SparkSession?\n",
    "Spark Session: A unified entry point for DataFrame and Dataset APIs.It's object \"spark\" is default available in spark-shell and it can be created programmatically using SparkSession builder pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe820d",
   "metadata": {},
   "source": [
    "In Apache Spark 2.x and later, the `SparkSession` is the entry point to any Spark functionality. When you want to run a Spark application, you first need to create a SparkSession. \n",
    "\n",
    "The `SparkSession.builder().getOrCreate()` method is a way to ensure that a SparkSession is created only once in an application.\n",
    "\n",
    "`SparkSession.builder()`: This returns a SparkSession.Builder object, which is a builder for a SparkSession. With the builder, you can configure options for the SparkSession, such as appName, master, and various Spark configurations using the config method.\n",
    "\n",
    "`getOrCreate()`: When called on a SparkSession.Builder object, this method: Retrieves the existing SparkSession if one already exists.\n",
    "Creates a new SparkSession if none exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2720229",
   "metadata": {},
   "source": [
    "### Create a DataFrame from a CSV file\n",
    "`read`:\n",
    "This is a method associated with SparkSession and it returns a DataFrameReader that can be used to read data. The read method provides functionality to read data from various sources into a Spark DataFrame.\n",
    "\n",
    "`option('header', 'true')`:\n",
    "The option method allows you to specify options when reading data. In this case, the option being set is 'header' with the value 'true'. This means that the first row of the CSV file (Traffic_Crashes_-_Crashes.csv) is considered as a header and will be used to name the columns of the DataFrame.\n",
    "\n",
    "If this option wasn't set (or set to 'false'), the CSV file would be read without considering the first row as a header, and default column names would be assigned (like _c0, _c1, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2680e5",
   "metadata": {},
   "source": [
    "**Q1: Load the data from the csv files into DataFrames.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c23646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the csv files into DataFrames.\n",
    "crashes = spark.read.option('header', 'true').csv('Traffic_Crashes_-_Crashes.csv')\n",
    "vehicles = spark.read.option('header' , 'true').csv('Traffic_Crashes_-_Vehicles.csv')\n",
    "peoples = spark.read.option( 'header', 'true').csv('Traffic_Crashes_-_People.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd44bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what is the type of crashes\n",
    "print(type(crashes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7355a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what is the data type of each DataFrame \n",
    "crashes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "peoples.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84cec8",
   "metadata": {},
   "source": [
    "In PySpark, the `pyspark.sql.types` module provides a collection of data types that you can use to specify the schema of a DataFrame. When you're working with data in Spark, sometimes you might need to explicitly define or cast data to a specific type. This is where these imports come into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9de2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql. types import StringType \n",
    "from pyspark.sql. types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6eb6b",
   "metadata": {},
   "source": [
    "**Q2: Find the ratio of number of crashes where the person involved was using cell phone to that where the person was not using the cell phone.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "peoples. groupby(peoples.DRIVER_ACTION).count().orderBy(\"count\").show(n=50,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35d458",
   "metadata": {},
   "source": [
    "#### In PySpark, when working with DataFrames, the `.show()` method is used to display the rows of the DataFrame in a tabular format, primarily for visual inspection during development or debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18e715",
   "metadata": {},
   "source": [
    "`truncate=False`: Content in each cell of the table will be displayed in full, regardless of its length. This means that if you have very long content in some cells, the display might stretch out horizontally, making it harder to read, but ensuring you see the full content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = peoples.groupby(peoples.DRIVER_ACTION).count().filter((peoples.DRIVER_ACTION == 'CELL PHONE USE OTHER THAN TEXTING') | (peoples.DRIVER_ACTION == 'TEXTING'))\n",
    "phone.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a553fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_crashes = phone.groupBy().sum('count').collect()[0][0]\n",
    "print (\"Crashes that occurs because of phone: \", phone_crashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_phone = peoples.groupby(peoples.DRIVER_ACTION).count().filter((peoples.DRIVER_ACTION != 'NONE') &(peoples.DRIVER_ACTION != 'UNKNOWN') & (peoples.DRIVER_ACTION != 'OTHER') & (peoples.DRIVER_ACTION != 'null') & (peoples.DRIVER_ACTION != 'CELL PHONE USE OTHER THAN TEXTING') &(peoples.DRIVER_ACTION != 'TEXTING'))\n",
    "no_phone.show(truncate=False)\n",
    "no_phone_crashes = no_phone.groupBy().sum('count').collect()[0][0]\n",
    "print (\"Crashes that occurs NOT because of phone: \", no_phone_crashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Ratio of phone to non-phone crashes\",phone_crashes,\"/\",no_phone_crashes, \"=\" , (100 * (phone_crashes / no_phone_crashes))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7405c",
   "metadata": {},
   "source": [
    "\n",
    "**Q3: Find which three Age groups were involved with highest number of crashes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4158319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to do mathematicl comparison or operations the variavle type should be numeric (int, double, float,...)\n",
    "# Here age is string which is text, we need to convert it to int first\n",
    "peoples = peoples.withColumn(\"AGE\", col(\"AGE\").cast(\"integer\"))\n",
    "peoples.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfa81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_groups = peoples.filter(~isnull(col(\"AGE\"))).groupBy(\"AGE\").count().orderBy(\"count\", ascending=False).limit(3)\n",
    "age_groups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7b7ae",
   "metadata": {},
   "source": [
    "**Q4. Find which month of the year has the highest crashes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes.select(\"CRASH_DATE\").show(n=100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, dayofweek, to_date, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fae22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes = crashes.withColumn(\"CRASH_DATE\", to_date(substring(crashes[\"CRASH_DATE\"], 1, 10), 'MM/dd/yyyy'))\n",
    "\n",
    "crashes.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes.select(\"CRASH_DATE\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e530e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crashes_month = crashes.withColumn(\"Month\", month(crashes[\"CRASH_DATE\"])).groupBy(\"Month\").count().orderBy(\"count\", ascending=False).limit(1)\n",
    "crashes_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac732a4",
   "metadata": {},
   "source": [
    "**Q5. Find which day of the week has the least crashes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3314913",
   "metadata": {},
   "source": [
    "In the dayofweek function in PySpark, the days of the week are represented as integers from 1 (Sunday) to 7 (Saturday). <br>\n",
    "1.Sunday <br>\n",
    "2.Monday<br>\n",
    "3.Tuesday<br>\n",
    "4.Wednesday<br>\n",
    "5.Thursday<br>\n",
    "6.Friday<br>\n",
    "7.Saturday<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ab257",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_day = crashes.withColumn(\"Day\", dayofweek(crashes[\"CRASH_DATE\"])).groupBy(\"day\").count().orderBy(\"count\", ascending=False).limit(1)\n",
    "crashes_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050095c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
