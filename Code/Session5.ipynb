{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da74b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Context\n",
    "from pyspark.sql import functions as func # To use 'sum', 'count', and other functions\n",
    "from pyspark.sql.types import IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fcd849",
   "metadata": {},
   "source": [
    "### Pandas vs PySpark\n",
    "The complexity of Data Processing Tasks: PySpark is more suitable for complex data processing tasks that involve multiple stages of data transformation and analysis. Pandas is more suitable for simple data analysis tasks that involve filtering, selecting, and aggregating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81989a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Python library to manage dataframes, similar as PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ad91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data = spark.read.option('header','true').options(delimiter=\";\").csv('bank.csv', inferSchema=True)\n",
    "bank_data.show()\n",
    "\n",
    "# NOTE:\n",
    "# What is inferSchema=True???\n",
    "# Infer schema will automatically guess the data types for each field. If we set this option to TRUE, the API will read some sample records from the file to infer the schema. If we want to set this value to false, we must specify a schema explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the type of each column\n",
    "\n",
    "bank_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e253d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the first 5 rows of data\n",
    "\n",
    "bank_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8d389",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fc880",
   "metadata": {},
   "source": [
    "### Calculate the Mean, Median, and Standard Deviation of all the variables/attributes of numeric type:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ab85a",
   "metadata": {},
   "source": [
    "**Getting a Database:** Once you have a connected aninstance of MngoClient, you can access any database managed by the specified MngoDB server. To define which database you want to use, you can use the dot notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e8f40",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdb6c8",
   "metadata": {},
   "source": [
    "### Using 'describe' function to provide the basic statisticsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.select('age', 'balance', 'day', 'duration', 'pdays', 'previous').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca695069",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83908737",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.select(func.stddev('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.select(func.mean('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a833bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.select(func.max('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.select(func.min('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of all columns\n",
    "bank_data.select([func.mean(c) for c in bank_data.columns]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd7866",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629ab4f",
   "metadata": {},
   "source": [
    "### Create a bar graph of the variables/attribute: Previous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cea962",
   "metadata": {},
   "source": [
    "#### We can  use the built-in functionality of Pandas to draw a chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc04827",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "ascending = False means from highest to lowest\n",
    "\n",
    "groupBy: It is used for grouping the data points (i.e. rows) based on the distinct values in the given column or columns. We can then calculate aggregated values for the generated groups.\n",
    "\n",
    "orderBy: Sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe in which the rows are groups based on the distict values of 'job' column\n",
    "df = bank_data.groupBy('job').sum().orderBy(\"sum(previous)\", ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8915e7f",
   "metadata": {},
   "source": [
    "#### Method 1: Using Pandas to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PySpark dataframe to Pandas dataframe ( to be able to use the Pandas chart drawing functionality)\n",
    "\n",
    "df.toPandas().plot.bar(x='job', y='sum(previous)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725be3cd",
   "metadata": {},
   "source": [
    "#### Method 2: Using Matplotlib to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74183b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar chart using Matplotlib\n",
    "df_pan = df.toPandas() #Convert the PySpark dataframe to Pandas dataframe\n",
    "\n",
    "plt.bar(df_pan['job'], df_pan['sum(previous)'], width=0.5, color=\"orange\")\n",
    " \n",
    "# Set the chart title and labels\n",
    "plt.title('Plot Title')\n",
    "plt.xlabel('job')\n",
    "plt.ylabel('sum(previous)')\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131a9af",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5275359",
   "metadata": {},
   "source": [
    "#### Create a Normalised bar graph of the variable/attribute: Previous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fef1c",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd5e35",
   "metadata": {},
   "source": [
    "**Normalization:** The goal of normalization is to transform features to be on a similar scale. This improves the performance and training stability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c50cf",
   "metadata": {},
   "source": [
    "Add Normalised columns to the inout dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a631af",
   "metadata": {},
   "source": [
    "#### Unsing the min-ax feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c9df",
   "metadata": {},
   "source": [
    "The min-max approach (often called **Normalization**) rescales the feature to a hard and fast range of [0,1] by subtracting the minimum value of the feature then dividing by the range. We can apply the min-max scaling in Pandas using the `.min()` and `.max()` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b9028",
   "metadata": {},
   "source": [
    "$$\n",
    " \\frac{x-min}{max-min}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50363e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df.toPandas()\n",
    "df_result[\"sum(previous)_norm\"] = (df_result[\"sum(previous)\"] - df_result[\"sum(previous)\"].min()) / (df_result[\"sum(previous)\"].max()-df_result[\"sum(previous)\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cff7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.plot.bar(x='job', y='sum(previous)_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd70b2",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccd0cd",
   "metadata": {},
   "source": [
    "#### Using z-score method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebeef8e",
   "metadata": {},
   "source": [
    "The z-score method (often called **Standardization**) transforms the info into a distribution with mean of 0 and the standard deviation of 1. each standardized value is computed by subtracting the mean of the corresponding feature whn dividing by the sandard deviaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058aa98",
   "metadata": {},
   "source": [
    "$$\n",
    " \\frac{x-mean}{stdev}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a543e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"sum(previous)_std\"] = (df_result[\"sum(previous)\"] - df_result[\"sum(previous)\"].mean()) / (df_result[\"sum(previous)\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.plot.bar(x='job', y='sum(previous)_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f77c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"sum(previous)_std\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104baecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[\"sum(previous)_std\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9e4f1",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09ad14",
   "metadata": {},
   "source": [
    "#### Create a histogram of the variable/attribute: age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bank_data.select('age') # Selecting the column 'age'\n",
    "df.toPandas().hist(column='age') # Converting the dataframe to Pandas and then draw a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77654f50",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115098d0",
   "metadata": {},
   "source": [
    "#### Create a histogram of the normalised variable/attribute: age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7198904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df.toPandas()\n",
    "df_result[\"age_norm\"] = (df_result[\"age\"] - df_result[\"age\"].min()) / (df_result[\"age\"].max()-df_result[\"age\"].min())\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9192ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.hist(column='age_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca239318",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4b1ff",
   "metadata": {},
   "source": [
    "#### Bin (groupby) the variable/attribute age and create a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02819830",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=bank_data.groupBy('job').agg(func.mean('age')).orderBy('avg(age)', ascending=False)\n",
    "df.toPandas().plot.bar(x='job', y='avg(age)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf96a2",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591878ad",
   "metadata": {},
   "source": [
    "#### Create a scatter plot for the following variable/attribute: age and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bank_data.select('age', 'balance')\n",
    "df.toPandas().plot.scatter(x='age', y='balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ce8d0",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4299702b",
   "metadata": {},
   "source": [
    "Consider a scenario in which the owner of the data set decides that any data under the ‘Job’ column that contains ‘unknown’ or ‘unemployed’ data should be considered inaccurate. Thus, those data should be removed from the dataset before data exploration.\n",
    "\n",
    "As part of this data removal operation (i.e., the data cleaning operation), you are requested to get rid of all the data rows in which the ‘Job’ column contains ‘unknown’ or ‘unemployed’ instead of data. You can remove these either using PySpark or Excel. Once these data have been removed, save the remaining data in a csv file named bank.csv as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eed3a",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b72957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'where' to filter the data\n",
    "bank_data.where(bank_data.job=='unemployed').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'and' (&) 'or' (|) to apply multiple conditions \n",
    "bank_data.where((bank_data.job=='unknown') | (bank_data.job=='unemployed')).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579608f",
   "metadata": {},
   "source": [
    "#### No of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140052a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data.where((bank_data.job=='unknown') | (bank_data.job=='unemployed')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d7589",
   "metadata": {},
   "source": [
    "#### Removing the unwanted rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dda754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = bank_data.where((bank_data.job!='unknown') & (bank_data.job!='unemployed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865646f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ac385",
   "metadata": {},
   "source": [
    "### Finding Duplicated Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bank_data.toPandas()\n",
    "duplicated_rows = df[df.duplicated()]\n",
    "print(duplicated_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde0dde",
   "metadata": {},
   "source": [
    "#### Writing the cleaned dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.write.csv(\"bank_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a241e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
