{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e9ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import scale \n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8164893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fake income/age clusters for W people in k clusters\n",
    "def createClusteredData (N, k) :\n",
    "    pointsPerCluster = float (N)/k\n",
    "    X = []\n",
    "    for i in range (k):\n",
    "        incomeCentroid = random.uniform(20000.0, 200000.0) # Draw samples from a uniform distribution.\n",
    "        # Samples are uniformly distributed over the half-open interval [Low, high) (includes low, but excludes high)\n",
    "        ageCentroid = random.uniform(20.0, 70.0)\n",
    "        X.append([incomeCentroid, ageCentroid])\n",
    "        \n",
    "        for j in range(int (pointsPerCluster) - 1):\n",
    "            # normal: Draw random samples from a normal (Gaussian) distribution.\n",
    "            # loc : Mean (\"centre\") of the distribution.\n",
    "            # scale: Standard deviation (spread or \"width\") of the distribution. Must be non-negative.\n",
    "            X.append([np.random.normal(loc = incomeCentroid, scale = 20000.0), np.random.normal(loc=ageCentroid, scale=2.0)])\n",
    "    X = np. array (X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a5dfe",
   "metadata": {},
   "source": [
    "**Why we should scale data?**\n",
    "\n",
    "\n",
    "Scaling data is important in many machine learning algorithms because it ensures that each feature contributes equally to the analysis. Here are the key reasons for scaling data:\n",
    "\n",
    "**Equalizing Feature Influence:** In many datasets, different features might be measured in different units and scales (e.g., kilograms, dollars, years). If one feature has a much larger scale than others, it can dominate the algorithm's behavior. Scaling normalizes these scales, so no single feature will unduly influence the model's outcome.\n",
    "Improving Algorithm Performance: Many machine learning algorithms, like KMeans clustering, use distance calculations. If features are on different scales, distance metrics can be skewed towards features with larger scales. Scaling ensures that the distance measure accurately reflects differences in all dimensions.\n",
    "Speeding Up Convergence: Algorithms that use gradient descent as an optimization technique (like neural networks and linear regression) converge faster when data is scaled. This is because scaling ensures a more balanced contribution of each feature to the loss gradient, avoiding erratic changes in the optimization path.\n",
    "Required by Some Models: Certain algorithms, like Support Vector Machines and Principal Component Analysis, require scaled data to function correctly. These algorithms are sensitive to the scale of the input data, and without scaling, they might not perform as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f304ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = createClusteredData(100, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = scale(data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].scatter(data[:, 0], data[:, 1])\n",
    "ax[0].set(xlabel ='Income', label= 'Age', title= \"Bank Data\")\n",
    "\n",
    "ax[1].scatter(scaled_data[:, 0], scaled_data[:, 1])\n",
    "ax[1].set(xlabel='Income', ylabel='Age', title=\"Scaled Bank Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1e835",
   "metadata": {},
   "source": [
    "\n",
    "### What is Inertia?\n",
    "\n",
    "Inertia, in the context of KMeans implemented in libraries like scikit-learn, is a measure that represents the total sum of squared distances of each data point to the centroid of its assigned cluster. \n",
    "\n",
    "It's essentially the value of the objective function.\n",
    "We would like this number to be as small as possible.\n",
    "\n",
    "K that is equal to the number of samples we will get inertia=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ead631",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = []\n",
    "for cluster in range(1,20):\n",
    "    model = KMeans(n_clusters = cluster, init='k-means++', n_init= 'auto') \n",
    "    model.fit(scaled_data)\n",
    "    SSE.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce427d77",
   "metadata": {},
   "source": [
    "#### SSE stands for Sum of Squared Errors\n",
    "which is a measure used to quantify the variance within the clusters formed by the KMeans algorithm.\n",
    "\n",
    "For each iteration of the loop, a KMeans model is created with the following parameters:\n",
    "**n_clusters = cluster:** The number of clusters to form, which changes with each iteration of the loop.\n",
    "\n",
    "**init='k-means++':** This parameter specifies the initialization method for the centroids. 'k-means++' is an optimized version of the KMeans algorithm that selects initial cluster centers in a smart way to speed up convergence.\n",
    "\n",
    "**n_init= 'auto':** This should be a numerical value indicating the number of times the algorithm will run with different centroid seeds. The final results will be the best output of these runs. The code might have a typo here, as 'auto' is not a valid value for n_init. Usually, a number like 10 is used.\n",
    "\n",
    "**model.fit(scaled_data):** This fits the KMeans model to the scaled_data. scaled_data should be a preprocessed version of your dataset where features are scaled to ensure equal contribution to the model (commonly using standardization or normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfcab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE' :SSE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o') \n",
    "plt.xlabel('Number of clusters') \n",
    "plt.ylabel('Inertia') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4)\n",
    "#note I am scaling data to normalise it! Important for good results\n",
    "model= model.fit(scale(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.labels_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e711f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56be082",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:,0], data[:,1], c = model.labels_.astype(np.int64))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
